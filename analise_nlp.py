import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import seaborn as sns
import os

# --- CONFIGURA√á√ÉO ---
ARQUIVO_DADOS = "dados_limpos_upas.csv"
PASTA_SAIDA = "resultados_nlp"

# Stopwords adicionais espec√≠ficas do contexto de sa√∫de que podem poluir a an√°lise
# Adicionei 'nao' aqui para remover a nega√ß√£o dos t√≥picos
STOPWORDS_EXTRAS = [
    'atendimento', 'upa', 'hospital', 'dia', 'hoje', 'fui', 'ser', 'pra', 't√°', 'vc', 
    'pq', 'vcs', 'ter', 'tinha', 'veio', 'disse', 'falou', 'falar', 'cheguei', 
    'ficar', 'gente', 'pessoal', 'lugar', 'bhorizonte', 'bh', 'minas', 'gerais'
]

def carregar_dados():
    if not os.path.exists(ARQUIVO_DADOS):
        print(f"‚ùå Erro: '{ARQUIVO_DADOS}' n√£o encontrado.")
        return None
    return pd.read_csv(ARQUIVO_DADOS).dropna(subset=['Texto_Limpo'])

def plotar_ngrams(df, n=2, qtd=15, titulo="N-Grams"):
    """Gera gr√°fico de barras com as sequ√™ncias de palavras mais comuns"""
    print(f"   > Calculando {titulo}...")
    
    # Configura o contador de palavras (Bi-gramas ou Tri-gramas)
    vec = CountVectorizer(ngram_range=(n, n), stop_words=STOPWORDS_EXTRAS).fit(df['Texto_Limpo'])
    bag_of_words = vec.transform(df['Texto_Limpo'])
    sum_words = bag_of_words.sum(axis=0) 
    
    # Lista de freq√º√™ncia
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    
    # Prepara dados para o gr√°fico
    top_words = words_freq[:qtd]
    df_plot = pd.DataFrame(top_words, columns=['Termo', 'Frequencia'])
    
    # Plota
    plt.figure(figsize=(12, 6))
    sns.barplot(x='Frequencia', y='Termo', data=df_plot, palette='rocket')
    plt.title(titulo, fontsize=16)
    plt.tight_layout()
    plt.savefig(f"{PASTA_SAIDA}/{titulo.replace(' ', '_').lower()}.png")
    plt.close()

def modelagem_topicos_lda(df, n_topicos=3, n_palavras=8):
    """
    Usa IA para descobrir os 3 assuntos principais nas reclama√ß√µes.
    """
    print(f"   > Executando LDA (Modelagem de T√≥picos) em {len(df)} textos...")
    
    # Vetoriza√ß√£o (Transforma texto em n√∫meros)
    vectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words=STOPWORDS_EXTRAS)
    dtm = vectorizer.fit_transform(df['Texto_Limpo'])
    
    # Cria o Modelo LDA
    lda = LatentDirichletAllocation(n_components=n_topicos, random_state=42)
    lda.fit(dtm)
    
    # Exibe e salva os t√≥picos
    plt.figure(figsize=(15, 5))
    feature_names = vectorizer.get_feature_names_out()
    
    # Cria um gr√°fico para cada t√≥pico
    for index, topic in enumerate(lda.components_):
        plt.subplot(1, n_topicos, index + 1)
        
        # Pega as palavras mais importantes do t√≥pico
        top_indices = topic.argsort()[-n_palavras:][::-1]
        top_features = [feature_names[i] for i in top_indices]
        top_weights = [topic[i] for i in top_indices]
        
        plt.barh(top_features, top_weights, color='#2ecc71')
        plt.gca().invert_yaxis()
        plt.title(f'T√≥pico {index + 1}', fontsize=14)
        plt.xlabel('Peso da Palavra')
    
    plt.suptitle('T√≥picos Ocultos Descobertos (LDA)', fontsize=16)
    plt.tight_layout()
    plt.savefig(f"{PASTA_SAIDA}/topicos_lda.png")
    plt.close()
    
    print("   > T√≥picos gerados. Verifique a imagem para interpretar o significado.")

def exportar_para_gephi(df):
    """
    Gera um arquivo CSV de arestas para ser importado no Gephi (Grafo de Co-ocorr√™ncia).
    Conecta palavras que aparecem na mesma frase.
    """
    print("   > Gerando arquivo para Gephi...")
    from itertools import combinations
    from collections import Counter
    
    todas_arestas = []
    
    # Pega apenas reclama√ß√µes (Nota <= 2) para o grafo ficar focado nos problemas
    reviews = df[df['Nota'] <= 2]['Texto_Limpo'].head(1000) # Limita a 1000 para n√£o travar
    
    for texto in reviews:
        palavras = [p for p in texto.split() if p not in STOPWORDS_EXTRAS and len(p) > 3]
        # Cria pares de palavras (arestas)
        pares = list(combinations(sorted(set(palavras)), 2))
        todas_arestas.extend(pares)
        
    # Conta frequ√™ncia das conex√µes
    contagem = Counter(todas_arestas)
    
    # Salva CSV
    with open(f"{PASTA_SAIDA}/gephi_arestas.csv", "w", encoding='utf-8') as f:
        f.write("Source;Target;Weight\n")
        for (source, target), weight in contagem.most_common(500): # Top 500 conex√µes
            if weight > 2: # Filtra conex√µes fracas
                f.write(f"{source};{target};{weight}\n")
                
    print(f"   > Arquivo 'gephi_arestas.csv' salvo. Importe isso no Gephi!")

def main():
    if not os.path.exists(PASTA_SAIDA):
        os.makedirs(PASTA_SAIDA)
        
    df = carregar_dados()
    if df is None: return
    
    print("--- üß† INICIANDO MINERA√á√ÉO DE TEXTO (NLP) ---")
    
    # Separa grupos
    ruins = df[df['Nota'] <= 2]
    bons = df[df['Nota'] >= 4]
    
    # 1. Bigramas em Reclama√ß√µes (O que as pessoas falam juntas?)
    plotar_ngrams(ruins, n=2, titulo="Bi-gramas nas Reclama√ß√µes (1-2 estrelas)")
    
    # 2. Trigramas em Reclama√ß√µes (Frases curtas)
    plotar_ngrams(ruins, n=3, titulo="Tri-gramas nas Reclama√ß√µes")
    
    # 3. Bigramas em Elogios (O que funciona?)
    plotar_ngrams(bons, n=2, titulo="Bi-gramas nos Elogios (4-5 estrelas)")
    
    # 4. Modelagem de T√≥picos (A m√°gica da IA)
    # Vamos pedir para ele achar 3 grandes problemas ocultos nas reclama√ß√µes
    modelagem_topicos_lda(ruins, n_topicos=3)
    
    # 5. Exportar para Gephi
    exportar_para_gephi(df)
    
    print(f"\n‚úÖ SUCESSO! Resultados salvos na pasta '{PASTA_SAIDA}'.")

if __name__ == "__main__":
    main()